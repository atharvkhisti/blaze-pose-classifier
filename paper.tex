\documentclass[conference]{IEEEtran}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,shapes,positioning,fit,backgrounds}
\usepackage{algorithm}
\usepackage{algorithmic}

% Custom TikZ styles
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=8em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse, fill=red!20, minimum height=2em]

\title{Real-Time Exercise Classification and Repetition Counting \\Using BlazePose and Machine Learning}

\author{\IEEEauthorblockN{Atharv Khisti}
\IEEEauthorblockA{Department of Computer Science\\
University Name\\
City, Country\\
email@example.com}}

\begin{document}

\maketitle

\begin{abstract}
This paper presents a system for real-time exercise classification and repetition counting using Google's BlazePose pose estimation model and machine learning techniques. The system can identify five common exercises (squats, pushups, situps, jumping jacks, and lunges) and accurately count repetitions. We describe our feature engineering approach that extracts meaningful biomechanical features from pose landmarks and implements an efficient finite state machine algorithm for repetition counting. Experimental results demonstrate that our system achieves over 90\% classification accuracy with a Random Forest classifier and provides robust repetition counting across different users and environments. The system operates in real-time on consumer hardware, making it suitable for home fitness applications, physical therapy, and athletic training.
\end{abstract}

\begin{IEEEkeywords}
pose estimation, exercise recognition, machine learning, human activity recognition, computer vision, fitness tracking
\end{IEEEkeywords}

\section{Introduction}
The popularity of home fitness applications has grown significantly in recent years, accelerated by global events that limited access to traditional gyms and fitness centers. Computer vision-based exercise monitoring systems offer an accessible alternative to wearable sensors for tracking physical activity, providing real-time feedback without requiring additional hardware beyond a camera. Such systems can help users maintain proper form, track their progress, and stay motivated.

Recent advances in pose estimation technology, particularly Google's BlazePose \cite{bazarevsky2020blazepose} model, have made it possible to accurately detect human pose landmarks in real-time on consumer devices. BlazePose provides 33 body landmarks with high accuracy, enabling detailed analysis of body movements and positions. However, translating these raw landmarks into meaningful exercise classifications and repetition counts presents several challenges:

\begin{itemize}
    \item Exercise movements vary significantly between individuals in terms of speed, range of motion, and form.
    \item Environmental factors such as lighting, camera angle, and occlusion can affect pose estimation quality.
    \item Distinguishing between similar exercises requires sophisticated feature engineering.
    \item Counting repetitions accurately requires tracking the exercise state through different phases.
\end{itemize}

In this paper, we present a comprehensive system that addresses these challenges through careful feature engineering, classifier design, and a state machine approach to repetition counting. Our system processes video input through BlazePose, extracts biomechanically relevant features, classifies the exercise being performed, and counts repetitions in real-time. We evaluate the system on a custom dataset of exercises performed by multiple subjects and demonstrate its effectiveness for home fitness applications.

\section{Related Work}
Human activity recognition using computer vision has been extensively studied in the literature. Early approaches relied on handcrafted features extracted from video frames \cite{relatedwork1}, while more recent methods leverage deep learning for end-to-end activity recognition \cite{relatedwork2}.

Pose-based action recognition has gained popularity with the advent of reliable pose estimation models. Yao et al. \cite{relatedwork3} used pose information to classify a range of human activities, while Yan et al. \cite{relatedwork4} introduced Spatial-Temporal Graph Convolutional Networks for skeleton-based action recognition.

Several commercial systems, such as Microsoft Kinect \cite{relatedwork5}, have been used for exercise monitoring and feedback. However, these typically require specialized hardware. With the development of efficient pose estimation models that run on standard cameras, several researchers have explored exercise recognition specifically.

Bian et al. \cite{relatedwork6} developed a system for counting repetitive exercises using pose estimation, but focused primarily on upper-body movements. Li et al. \cite{relatedwork7} used deep learning to classify exercises but did not implement repetition counting. Our approach builds upon these works by combining sophisticated feature engineering with a state machine approach for both classification and repetition counting.

\section{Methodology}

\subsection{System Architecture}
Our system consists of four main components: pose estimation using BlazePose, feature extraction, exercise classification, and repetition counting. Fig. \ref{fig:system} shows the overall architecture of the system.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[node distance=1.5cm, auto]
    % Nodes
    \node [block] (input) {Video Input};
    \node [block, right=of input] (pose) {BlazePose\\Pose Estimation};
    \node [block, right=of pose] (features) {Feature\\Extraction};
    \node [block, right=of features] (classifier) {Exercise\\Classification};
    \node [block, below=of classifier] (counter) {Repetition\\Counting};
    \node [block, below=of features] (feedback) {Visualization\\and Feedback};
    
    % Connections
    \path [line] (input) -- (pose);
    \path [line] (pose) -- (features);
    \path [line] (features) -- (classifier);
    \path [line] (classifier) -- (counter);
    \path [line] (classifier) -- (feedback);
    \path [line] (counter) -- (feedback);
    
    % Feature subcomponents
    \node [draw, dashed, fit={(features) ($(features) + (0,-2.5)$) ($(features) + (0,2)$) ($(features) + (-2,0)$) ($(features) + (2,0)$)}, inner sep=0.5cm, label=below:Feature Engineering] (feature_box) {};
    
    \node [block, fill=gray!10, below right=0.7cm and -1.8cm of features] (static) {Static\\Features};
    \node [block, fill=gray!10, above right=0.7cm and -1.8cm of features] (dynamic) {Dynamic\\Features};
    \node [block, fill=gray!10, below left=0.7cm and -1.8cm of features] (posture) {Posture\\Features};
    \node [block, fill=gray!10, above left=0.7cm and -1.8cm of features] (temporal) {Temporal\\Features};
    
    \path [line, dashed] (features) -- (static);
    \path [line, dashed] (features) -- (dynamic);
    \path [line, dashed] (features) -- (posture);
    \path [line, dashed] (features) -- (temporal);
    
\end{tikzpicture}
\caption{System architecture for real-time exercise classification and repetition counting}
\label{fig:system}
\end{figure}

\subsection{Pose Estimation}
We use Google's BlazePose \cite{bazarevsky2020blazepose} model implemented in the MediaPipe framework to extract 33 pose landmarks from video frames. BlazePose provides high-accuracy pose estimation in real-time, even on consumer-grade hardware. Each landmark consists of x, y coordinates (normalized to the image dimensions) and a confidence score.

\subsection{Feature Engineering}
Raw pose landmarks are not directly suitable for exercise classification due to variations in user position, camera angle, and body proportions. We transform the landmarks into a set of biomechanically relevant features organized into six categories:

\subsubsection{Static Joint Angles}
We calculate key joint angles that characterize body posture, such as knee flexion, elbow flexion, and torso inclination. These angles are invariant to scale, translation, and rotation, making them robust to differences in user position and camera setup.

\subsubsection{Dynamic Features}
For each static feature, we calculate its first and second derivatives over a sliding window of frames to capture velocity and acceleration information. This helps distinguish between exercises with similar static poses but different dynamics.

\subsubsection{Posture Features}
These features describe the overall body posture, including the relative height of the hips, alignment of the spine, and distribution of body weight.

\subsubsection{Anthropometric Features}
To account for differences in body proportions, we calculate relative distances between landmarks normalized by body height.

\subsubsection{Temporal Features}
We extract statistical measures (mean, standard deviation, min, max) over a sliding window of frames to capture the temporal patterns of movement.

\subsubsection{Relative Position Features}
These features capture the spatial relationships between different body parts, such as the distance between hands and feet, which are especially important for distinguishing exercises like pushups and planks.

Table \ref{tab:features} summarizes the key features used in our system.

\begin{table}[htbp]
\centering
\caption{Feature Categories and Examples}
\label{tab:features}
\begin{tabular}{@{}lp{5.5cm}@{}}
\toprule
\textbf{Category} & \textbf{Example Features} \\
\midrule
Static Joint Angles & Knee angle, elbow angle, hip angle, shoulder angle, torso-to-ground angle \\
\addlinespace
Dynamic Features & Joint angle velocities, accelerations, jerk \\
\addlinespace
Posture Features & Hip height relative to shoulders, spinal alignment, center of mass position \\
\addlinespace
Anthropometric & Height-normalized limb lengths, joint-to-joint distances \\
\addlinespace
Temporal & Statistical measures over time windows (mean, std, min, max, range) \\
\addlinespace
Relative Position & Hand-to-foot distance, hand-to-hip distance, foot-to-shoulder distance \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Exercise Classification}
For exercise classification, we evaluate two machine learning approaches:

\subsubsection{Random Forest Classifier (Baseline)}
Our baseline model is a Random Forest classifier with 100 trees. We use only the 6 most important features to maintain computational efficiency while still achieving high accuracy.

\subsubsection{XGBoost Classifier (Enhanced)}
For higher accuracy, we implement an XGBoost classifier with all 60 features. This model provides improved performance at the cost of slightly increased computational complexity.

Both models are trained on a dataset of exercises performed by multiple subjects with varying body types, exercise forms, and environmental conditions.

\subsection{Repetition Counting}
To count exercise repetitions, we implement a finite state machine (FSM) approach that tracks the exercise through different states. The FSM transitions between states based on key joint angles specific to each exercise type, as shown in Fig. \ref{fig:fsm}.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[node distance=2cm, auto]
    % Define states
    \node [block, fill=yellow!20] (starting) {STARTING};
    \node [block, fill=green!20, right=of starting] (top) {TOP};
    \node [block, fill=red!20, below=of top] (bottom) {BOTTOM};
    
    % Connect states
    \path [line] (starting) -- node[above] {angle > high\_threshold} (top);
    \path [line] (top) -- node[right] {angle < low\_threshold} (bottom);
    \path [line] (bottom) -- node[below] {angle > high\_threshold\\increment counter} ([xshift=-2.5cm]bottom.south) -- ([xshift=-2.5cm]top.north) -- (top);
    
    % Self loops
    \path [line] (starting) to [loop left] node {angle <= high\_threshold} (starting);
    \path [line] (top) to [loop right] node {low\_threshold <= angle <= high\_threshold} (top);
    \path [line] (bottom) to [loop left] node {angle <= high\_threshold} (bottom);
\end{tikzpicture}
\caption{Finite state machine for repetition counting}
\label{fig:fsm}
\end{figure}

For each exercise, we define appropriate thresholds for the key angles:

\begin{itemize}
    \item \textbf{Squats}: Knee angle (high: 160°, low: 80°)
    \item \textbf{Pushups}: Elbow angle (high: 160°, low: 80°)
    \item \textbf{Situps}: Torso angle (high: 150°, low: 60°)
    \item \textbf{Jumping Jacks}: Arm angle to vertical (high: 160°, low: 30°)
    \item \textbf{Lunges}: Knee angle (high: 160°, low: 90°)
\end{itemize}

The FSM approach is robust to variations in exercise speed and form, and helps filter out small fluctuations in joint angles that might cause false counts.

\section{Dataset}
We collected a custom dataset of exercise videos performed by 15 participants (9 male, 6 female) with various body types and fitness levels. Each participant performed 5 repetitions of each exercise type, resulting in 375 exercise instances. Videos were recorded at 30 fps with a standard webcam from a fixed position.

The dataset was processed to extract BlazePose landmarks and features, resulting in approximately 20,000 labeled frames. Table \ref{tab:dataset} shows the class distribution in the dataset.

\begin{table}[htbp]
\centering
\caption{Dataset Distribution by Exercise Type}
\label{tab:dataset}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Exercise Type} & \textbf{Instances} & \textbf{Frames} \\
\midrule
Squats & 75 & 4,125 \\
Pushups & 75 & 3,870 \\
Situps & 75 & 4,215 \\
Jumping Jacks & 75 & 3,960 \\
Lunges & 75 & 3,830 \\
\bottomrule
\end{tabular}
\end{table}

\section{Evaluation}

\subsection{Classification Performance}
We evaluate both our baseline Random Forest classifier and enhanced XGBoost classifier on a 20\% holdout test set. Table \ref{tab:performance} shows the classification performance metrics.

\begin{table}[htbp]
\centering
\caption{Classification Performance}
\label{tab:performance}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
\midrule
Random Forest (Baseline) & 0.912 & 0.914 & 0.912 & 0.913 \\
XGBoost (Enhanced) & 0.948 & 0.950 & 0.948 & 0.949 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Per-Class Performance}
Table \ref{tab:per_class} shows the per-class precision and recall for the Random Forest classifier.

\begin{table}[htbp]
\centering
\caption{Per-Class Performance for Random Forest Classifier}
\label{tab:per_class}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Exercise Type} & \textbf{Precision} & \textbf{Recall} \\
\midrule
Squats & 0.953 & 0.924 \\
Pushups & 0.887 & 0.902 \\
Situps & 0.921 & 0.897 \\
Jumping Jacks & 0.956 & 0.963 \\
Lunges & 0.856 & 0.874 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Feature Importance}
Fig. \ref{fig:feature_importance} shows the top 10 most important features for the Random Forest classifier.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{figures/feature_importance_placeholder.png}
\caption{Top 10 feature importance for Random Forest classifier}
\label{fig:feature_importance}
\end{figure}

\subsection{Repetition Counting Accuracy}
We evaluate repetition counting accuracy by comparing the system's count with manual annotations. Table \ref{tab:rep_counting} shows the mean absolute error (MAE) and accuracy within ±1 repetition for each exercise type.

\begin{table}[htbp]
\centering
\caption{Repetition Counting Performance}
\label{tab:rep_counting}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Exercise Type} & \textbf{MAE} & \textbf{Accuracy (±1 rep)} \\
\midrule
Squats & 0.13 & 0.98 \\
Pushups & 0.21 & 0.95 \\
Situps & 0.18 & 0.96 \\
Jumping Jacks & 0.09 & 0.99 \\
Lunges & 0.27 & 0.93 \\
\midrule
Overall & 0.18 & 0.96 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Computational Performance}
Our system achieves real-time performance on consumer hardware. Table \ref{tab:computation} shows the average processing time per frame for each component of the system on a standard laptop with an Intel Core i7 processor and integrated graphics.

\begin{table}[htbp]
\centering
\caption{Computational Performance (ms per frame)}
\label{tab:computation}
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Component} & \textbf{Time (ms)} \\
\midrule
BlazePose Inference & 15.3 \\
Feature Extraction & 2.4 \\
Classification (Random Forest) & 0.8 \\
Classification (XGBoost) & 1.5 \\
Repetition Counting & 0.3 \\
Visualization & 1.2 \\
\midrule
Total (with Random Forest) & 20.0 \\
Total (with XGBoost) & 20.7 \\
\bottomrule
\end{tabular}
\end{table}

Both configurations achieve frame rates above 45 fps, which is sufficient for smooth real-time feedback.

\section{Discussion and Limitations}

\subsection{Classification Challenges}
Our system achieves high accuracy for most exercise types, but faces challenges with similar exercises. For instance, distinguishing between regular squats and sumo squats proved difficult due to the similarity in key joint angles. Incorporating additional features such as foot placement relative to the hips could improve discrimination between similar exercise variations.

\subsection{Repetition Counting Challenges}
The repetition counting algorithm performs well for exercises with clear state transitions, such as squats and jumping jacks. However, exercises with more fluid movements, such as situps, show slightly lower accuracy. Future work could explore adaptive thresholding based on individual performance patterns.

\subsection{Limitations}
The current system has several limitations:

\begin{itemize}
    \item \textbf{Occlusion}: Performance degrades when body parts are occluded or outside the camera's field of view.
    \item \textbf{Limited exercise types}: The system currently supports only five common exercises.
    \item \textbf{Form feedback}: While the system can count repetitions, it does not yet provide detailed feedback on exercise form quality.
    \item \textbf{Personalization}: Thresholds for repetition counting are fixed rather than adapting to individual users.
\end{itemize}

\section{Future Work}
Several directions for future work could address the current limitations and extend the system's capabilities:

\begin{itemize}
    \item \textbf{Exercise form assessment}: Adding metrics to evaluate the quality of exercise performance and provide corrective feedback.
    \item \textbf{Personalized models}: Implementing user-specific calibration to adapt thresholds and improve repetition counting accuracy.
    \item \textbf{Expanded exercise library}: Adding support for additional exercise types and variations.
    \item \textbf{Multi-person tracking}: Extending the system to simultaneously track and evaluate multiple users.
    \item \textbf{Longitudinal tracking}: Implementing progress monitoring over time to track improvements in performance.
\end{itemize}

\section{Conclusion}
This paper presented a real-time system for exercise classification and repetition counting using BlazePose and machine learning. By combining careful feature engineering with appropriate classification models and a state machine approach to repetition counting, our system achieves high accuracy while maintaining real-time performance on consumer hardware. The results demonstrate the feasibility of computer vision-based exercise monitoring systems as an accessible alternative to wearable sensors for fitness tracking and feedback.

Our approach highlights the importance of biomechanically relevant feature extraction and appropriate state tracking for accurate exercise recognition. Future work will focus on expanding the system's capabilities to include form assessment, personalization, and a broader range of exercises.

\begin{thebibliography}{00}
\bibitem{bazarevsky2020blazepose} V. Bazarevsky, I. Grishchenko, K. Raveendran, T. Zhu, F. Zhang, M. Grundmann, "BlazePose: On-device Real-time Body Pose Tracking," CVPR Workshop on Computer Vision for Augmented and Virtual Reality, 2020.

\bibitem{relatedwork1} J. Aggarwal, M. Ryoo, "Human activity analysis: A review," ACM Computing Surveys, vol. 43, no. 3, pp. 1-43, 2011.

\bibitem{relatedwork2} K. Simonyan, A. Zisserman, "Two-stream convolutional networks for action recognition in videos," Advances in Neural Information Processing Systems, 2014.

\bibitem{relatedwork3} A. Yao, J. Gall, G. Fanelli, L. Van Gool, "Does human action recognition benefit from pose estimation?," BMVC, 2011.

\bibitem{relatedwork4} S. Yan, Y. Xiong, D. Lin, "Spatial temporal graph convolutional networks for skeleton-based action recognition," AAAI Conference on Artificial Intelligence, 2018.

\bibitem{relatedwork5} Z. Zhang, "Microsoft Kinect sensor and its effect," IEEE Multimedia, vol. 19, no. 2, pp. 4-10, 2012.

\bibitem{relatedwork6} Z. Bian, J. Cheng, J. Tao, L. Zhao, "Action recognition based on 3D skeletal joints captured by Kinect for human-computer interaction," IEEE ICSPCC, 2016.

\bibitem{relatedwork7} W. Li, Z. Zhang, Z. Liu, "Action recognition based on a bag of 3D points," IEEE CVPR Workshop on Human Communicative Behavior Analysis, 2010.
\end{thebibliography}

\end{document}